## 数据预处理：ML 中最重要的步骤

---

没有好的数据，即使你的模型再强大也没用。就比如一个很聪明的人要学习，你不给他书本，材料，案例，经验，或者给他错误的材料和案例，那只能让这个聪明人变得更可怕而已。

### 特征工程：维度诅咒

为什么要进行特征工程，最主要的原因就是维度诅咒的存在。资源有限，不可能使用所有的数据特征来进行拟合，所以要选择最重要的特征，也就是维度。

次元递减，和降维技术就至关重要。可以提高性能，降低过拟合风险，减少计算成本等。

主要的**特征选择**方法：

根据数据集的特征，数据数量等，可以选择前向选择，后向选择等方法进行特征选择。

比如数据本来就不多，那可以后向选择，先用所有的特征进行拟合，然后去掉不是很重要的特征，继续下一轮试验等。

主要的**特征提取**方法：

PCA 主成分分析，K-means，SVD 奇异值分解降维技术。

### 缺失数据的处理

也就是 impute missing data。

1. 如果只是为了玩数据，那么甚至可以直接删掉有缺失的数据，但是删除非常不好，因为如果删除了重要数据，可能会产生很大的副作用。

2. 均值替换和中值替换。这是最常用的方法，经常一行 python 就可以解决。均值替换简单而迅速但是其实很不负责，因为有异常数据存在的话，会对均值产生很大偏差。所以中值替换更常用。但是中值也有缺点：特征之间有相关性的时候，以及分类数据中，用中值是很不好的选择。一般也就是回归问题会用。

3. ML 算法补全数据：KNN（使用 k 临近算法找到相似数据进行补全），DL（深度学习算法），Regression（回归模型补全），MICE（利用链式方程进行多重插补，形成多组数据，应对不确定性）。

4. 最好的方法永远是寻找和收集更多的数据。

### 失衡数据处理

也就是 unbalanced 数据。

主要有两个手法，记住**采样**和**阈值**这两个关键词。

采样上一个过采样（oversampling），一个欠采样（undersampling），positive 的类别标签很少的情况，你很难预测到好的结果。一种方法是对量少的标签，采取过采样方法，复制样本也好，生成合成样本（SMOTE 技术，在空间向量中找到和目标标签相似的数据进行合成）也好，生成噪声样本也好，总之就是增加量少的标签的数量，使得它和另外一类的数量，不要差别那么大。相反的就是欠采样，也就是减少量大的标签的数量。但是你删除数据总是不好的，那可是信息啊，所以更多的情况还是使用过采样技术。

阈值调整方面，就是降低预测的阈值。因为分类问题实质上是对概率的计算，只要调低小概率事件的发生的上限概率阈值。就能改善结果。打比方我们一般以 0.5 为阈值决定正反面的选择，那么调整为比如 0.2， 那么小概率事件的发生，基于大于概率 0.2 就判断它为正，就可以了。

### 异常值处理

也就是 handling outliers。

什么是 outliers。也就是超出了方差和标准差的某个范围标准的那些奇怪的值。

通过方差和标准差的计算，可以看出数据的大体分布，距离这个分布太远的值会对数据造成奇怪的影响。通过一个 python 的函数就可以处理。

example: 通过引入一个过滤器，排除阈值之外的异常值。

```python
import numpy as np
incomes = np.random.normal(27000, 15000, 10000)
incomes = np.append(incomes, [1000000000])

def reject_outliers(data):
    u = np.median(data)
    s = np.std(data)
    filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]
    return filtered

filtered = reject_outliers(incomes)
```

## 循环神经网络（RNN）

---
### 1，混淆的一个概念的澄清

RNN的R是Recurrent的意思是循环复发的意思，另外还有一个叫RNN的R事Recursive的意思，是一种递归的神经网络，他和循环神经网路哦不是一个东西，是用于处理树结构的神经网络。


### 循环神经元：时光旅行的起点

RNN的核心是循环神经元，它就像是一个携带着记忆的时间旅行者。每当接收新的输入时，循环神经元会运用激活函数（通常是Tanh双曲正切函数），并将上一时刻的输入合并进学习数据。这就好比是一个输入自循环的过程，或者说是在时间轴上不断利用先前所有输入数据的演变。

循环神经元内部有一个称之为记忆单元的机制，它随着时间的推移将更早的输入影响逐渐稀释，而最近的输入影响则更为显著。这种机制使得RNN能够更好地处理序列数据，比如**股票价格、机器翻译、语音、词序分析以及机器生成音乐**等。

### RNN的拓扑结构：时光的交织

RNN的拓扑结构是一个时光的交织，通过不同的连接方式形成多样的应用场景：

- **Sequence to Sequence（序列到序列）：**
  - 例如，基于历史数据分析股票价格。RNN可以捕捉先前时刻的信息，对未来的走势进行预测。

- **Sequence to Vector（序列到向量）：**
  - 从句子中提取情绪分析，将一个序列映射到一个固定维度的向量表示。

- **Vector to Sequence（向量到序列）：**
  - 从图像提取标题，将一个固定维度的向量转换为一个序列。

- **Encode to Decode（编码到解码）：**
  - 机器翻译是一个典型例子，将输入句子编码为一个向量，再解码为目标语言的句子。

### 训练难度：时光逆行的挑战

尽管RNN在处理时间序列上表现出色，但训练过程中也面临着一系列挑战：

- **梯度消失和梯度爆炸：**
  - 时间序列的反向传播时，梯度可能会迅速减小（梯度消失）或迅速增大（梯度爆炸），导致训练困难。
  - 每次进行计算的时候，过去的数据会共享现在的数据的权重，乘以这个weight，当weight大于一的时候，数据会不断的增大，导致梯度爆炸，反之，当weight小于一的时候，输入数据会越来越小无限接近于零，最终导致梯度消失，由于这个问题，出现了LSTM。

- **长短时记忆（LSTM）：**
  - 为了解决梯度消失和梯度爆炸的问题，LSTM引入了长期和短期记忆的机制，适用于自然语言处理、时间序列预测等任务。
  - 知识点包括：
    - long term memory with cell state：长期记忆的内容没有权重更新影响，也就是没有weight。
    - short term memory with hidden state：短期记忆内容是有权重更新的。
    - 短期记忆的input经过权重处理和激活函数（sigmoid输出[0, 1]）的计算，完成第一个stage的处理，这个处理出来的一个结果是一个百分比，这个百分比你决定了**有多少long-term-memory会被保留下来**。这个stage的功能也被叫做**forget gate**忘却门，虽然表达的是有多少被记忆，但是其实也是决定有多少被忘却。
    - 在第二个stage的处理中，继续对短期记忆的内容进行处理，得到**潜在长期记忆（tanh激活函数）**和**潜在长期记忆百分比（sigmoid激活函数）**，将两者进行相乘，得到当前的短期记忆是否会成为长期记忆，以及有多少内容成为长期记忆。这个stage的功能也被叫做**input gate**记忆门。
    - 在第三个stage的处理中，使用之前使用了两次的sigmoid激活函数得到有多少的长期记忆会被转化为短期记忆的百分比，最终计算得到输出。这个stage又被称为**output gate**输出门。

    - LSTM相对于普通的RNN，能够更有效地解决长期依赖（long-term dependency）问题，其有效性主要归因于以下几个方面：

    1. **记忆单元（Memory Cell）**：LSTM引入了一个称为记忆单元的结构，该单元可以存储信息并在长时间跨度上保持这些信息。通过精心设计的门控结构，记忆单元可以决定何时记忆、读取或清除信息，从而有效地管理和控制信息的流动，有助于解决长期依赖问题。

    2. **门控结构**：LSTM包含三种门：遗忘门（forget gate）、输入门（input gate）和输出门（output gate）。这些门控制着信息流的传递和遗忘，使得网络可以根据输入数据的特征自适应地选择性地记忆或遗忘信息。通过这种门控机制，LSTM可以有效地处理不同时间步长上的信息，从而更好地捕捉时间序列数据中的长期依赖关系。

    3. **梯度传播**：由于LSTM中门控单元的存在，网络的误差可以在时间上更有效地传播。相比于普通的RNN，LSTM中的梯度可以更容易地在时间上保持稳定，避免了梯度消失或梯度爆炸问题，有助于提高网络的训练效率和性能。

    4. **灵活性**：LSTM网络具有很高的灵活性，可以适应各种时间序列数据的特征和模式。通过调整记忆单元的大小、门控结构的参数等，可以根据具体任务的需求对网络进行调整和优化，从而获得更好的性能。

    总的来说，LSTM之所以能够起作用，主要是因为它通过记忆单元和门控结构有效地解决了长期依赖问题，同时具有良好的梯度传播性质和灵活性，使得它在处理各种时间序列任务中表现出色。

- **门控循环单位（GRU）：**
  - GRU是LSTM的简化版本，通过门控机制实现信息的更新，同样适用于各种序列处理任务。

在训练RNN时，选择合适的拓扑结构和参数至关重要。错误的选择可能导致网络无法收敛，成为一项艰难的任务。

总的来说，RNN如同时光旅行者，穿越序列的脉络，为处理时间相关数据的任务带来了新的可能性。然而，也正因为时光的复杂性，我们在训练过程中需要不断迭代、探索，以更好地理解和利用这个强大的神经网络。

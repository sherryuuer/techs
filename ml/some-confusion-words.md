## Scaling（缩放）、Standardization（标准化）、Normalization（归一化）和Regularization（正则化）

---

这几个东西经常让人混淆，所以需要一个小结。

当涉及到数据预处理和机器学习模型训练时，有几个重要的概念需要理解：Scaling（缩放）、Standardization（标准化）、Normalization（归一化）和Regularization（正则化）。以下是对这几个概念的详细解释：

### 1. Scaling（缩放）

Scaling是调整特征的数值范围，以便更好地适应模型的过程。主要目标是确保所有特征具有相似的尺度。

常见的缩放方法有Min-Max Scaling和Z-Score Scaling。

- 最小-最大缩放Min-Max scaling也是一种归一化，我们对数据进行变换，使特征处于特定范围内，例如 [-1, 1]。 对于需要保留原始数据分布形状的场景较为合适
- 均值归一化（Mean Normalization），使用均值，库是标准化器Normalizer，当数据包含异常值时，使用 Robust 缩放更为合适。
- 标准化Standardization——StandardScalar库，均值为0，上下波动为1——Z-Score 标准化，当数据分布接近正态分布时较为适用，或者对模型的假设要求为标准正态分布时。

用途： 缩放有助于防止某些特征对模型产生过大的影响，提高模型的收敛速度和稳定性。

### 2. Standardization（标准化）

标准化是将数据转换为标准正态分布（均值为0，标准差为1）的过程。它是Scaling的一种形式，通过减去均值并除以标准差实现。就是上面Scalling的第三个东西。

标准化有助于确保不同特征的权重更新在相似的尺度上，提高模型的鲁棒性和泛化性能。

### 3. Normalization（归一化）

归一化是将数据映射到指定范围（通常是[0, 1]）的过程。与Scaling不同，归一化是特定尺度的缩放过程。

可以用上面最小最大缩放的方法，他也是一种缩放方法。

归一化确保数据在特定范围内，并且对于涉及距离度量的模型（如K近邻），有助于提高性能。

### 4. Regularization（正则化）

正则化是通过在损失函数中引入额外项，对模型参数进行惩罚的过程。目的是防止过拟合，提高模型的泛化性能。

常见的正则化项包括L1正则化和L2正则化。

正则化有助于控制模型的复杂度，防止过拟合，提高模型在未知数据上的泛化性能。这在过拟合那一篇md中也有说。

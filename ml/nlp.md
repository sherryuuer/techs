## Tensorflow 自然语言处理流程

---

### 自然语言处理的主要应用领域

机器翻译，文本处理，情感分析，文本分类。

### 在 tf 中的主要处理流程是什么

- 数据准备
- 数据处理，词向量化，词嵌入
- 建模（包括从 hub 提取好用的迁移模型）
- 编译模型
- 拟合模型
- 通过测试集预测和查看结果
- 更多的模型试验和最终模型比较
- 查看错误的预测，考虑如何通过标签修改提升模型性能
- 应用部署

### 数据前置处理很重要

_防止数据泄漏_

在学习中，第一次将数据打乱后，从打乱的数据中提取训练数据，很容易把测试数据也混淆进训练数据，造成数据泄漏，以至于训练后的结果异常的好。有时候不要在好的预测精度上高兴的太早，预测结果是否合理，要提前心里有数。

所以在数据分割上要做好功课，先分割，提前分割，尽早分割，然后将 test 集放在一边就不要再动了，直到需要测试的时候，再拿出来用。

活用 sklearn 的分割方法：

```python
from sklearn.model_selection import train_test_split
train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled["text"].to_numpy(),
                                                                            train_df_shuffled["target"].to_numpy(),
                                                                            test_size=0.1,
                                                                            random_state=42)
```

_数据可视化_

即使是文本数据，也有必要进行可视化的处理，提前查看，文本的长度，内容，标签，是否正常。

例如：

```python
import random
random_index = random.randint(0, len(train_df)-5)
for row in train_df_shuffled[["text", "target"]][random_index:random_index+5].itertuples():
  _, text, target = row
```

查看全文本数据量，查看全标签是否不平衡。

是否有缺损数据。如果是 kaggle 的数据，大概率没有什么缺损可以直接拿来用。但是生产环境，就需要进行缺损数据的补全，有时候甚至需要机器学习算法来进行更好的补全。

### 词向量化和词嵌入

对于计算机来说，一切都是数字，所以 nlp 也不例外。

词向量是NLP中的基础概念之一，它通过非监督学习方式获取词汇之间的相关性信息。以角度的方式测量词向量之间的距离，能够告诉我们词的频率性特征。

词转数字就是词向量化，很好理解，最简单的向量化，就像是 one-hot 的方法，给每个单词一个数字，然后交给一个向量，给他维度。

词嵌入可以理解为，不只是看一个单词，而是解析单词和单词之间的关系的矩阵，同时还有另一个矩阵在内部作用，context 矩阵，该矩阵装着上下文信息，通过每一个批次的处理，使得两个矩阵的数字不断调整，让关系更近的词汇更近，更远的词汇更远。通过学习，提高模型精度。

### 1 维 CNN 在处理时间序列和序列化数据中表现出色

虽然说 CNN 一说起来就是图像识别领域，但是在序列化数据中，使用 1 维的 CNN 模型表现也非常出色。

### 不要小看 sk 的模型

基线模型朴素贝叶斯只是用了 tf-idf 处理单词，表现就非常突出，在基础构架中，打败它的只有迁移学习模型 USE。

**TF-IDF算法**简单来说，就是词频和逆文档频率的乘积。词频主要是指token在单独文档中出现的频率，逆文档频率主要是指在整个文档集中出现的频率。

如果一个单词在一个文档中多次出现，那么它可能比较重要，但是如果在整个文档集中都普遍出现那么可能就没那么重要了。IDF的计算，是总文档数/包含该词的文档数（+1防止除数为0）取对数，如果出现的越多，idf就越小，最后的结果就越小，反之越高，那么这个词在那篇文档中的地位就越重要了。

有时候这个算法不单单用于单独单词，还可能是bigrams两个单词，或者trigrams三个单词一组的。

通过该算法可以模拟建设一个小型的搜索引擎。

用tfidf实现搜索的几个步骤：

- **TF-IDF的cos距离：** 计算文章和问句的TF-IDF值，通过cos距离比较向量的夹角，实现文本搜索。
- **关键词提取：** 通过TF-IDF得分最高的词语，提取关键词。
- **企业级并行搜索：** Elastic Search等工具的应用，利用BM25算法进行集群搜索。

关于sklearn的稀疏矩阵：

- **sklearn的使用：** 使用sklearn库进行TF-IDF算法的实现，稀疏矩阵的存储优化。
- **稀疏矩阵：** 具有更多零元素的矩阵，在存储和处理上具有一些优势，适用于大规模数据集。

#### word2vec：词语特征的奇迹

- **直接应用：** word2vec可以将词向量当成词语特征输入到另一个模型中，实现多种任务，如情感分析、命名实体识别等。
- **加减法运算：** 还能进行加减法运算，通过计算相似性找到相近词语。
- **余弦相似度和负采样：** 利用余弦相似度衡量词向量之间的相似性，同时通过负采样机制提高训练效率。

#### CBOW与Skip-Gram

- **CBOW（Continuous Bag-of-Words）：** 类似于一个连续的词袋，通过前后文窗口移动训练，预测目标是中间的词。但其空间上的向量相加在直观上存在问题，通常被当做预训练模型，用于进一步训练句向量。
- **Skip-Gram：** 在理解上更为合理，通过一个词预测其前后的词，适用于一词多义的情况。

#### 两个矩阵：嵌入矩阵与上下文矩阵

词向量的表示通常使用两个矩阵，嵌入矩阵（embedding）和上下文矩阵（context），共同构建出词汇的丰富语义信息。

### 句向量：时间序列的循环奇迹

利用循环神经网络（RNN）进行训练！

- **Encoding/Decoding：** RNN以其擅长的时间序列预测能力，通过Encoding和Decoding过程实现对句向量的学习。
- **Seq2Seq：** 在NLP中，Seq2Seq模型利用RNN进行向量化和顺序理解，适用于翻译、情感分析、对话生成等任务。
- **Beam Search：** 通过关注当前候选词的N个最优策略，提高找到全局最优策略的概率。

另外这三个模型真的挺好玩的 LSTM，GRU，Bidirectional。

LSTM 是回忆过去的记忆，GRU 是控制是否遗忘记忆，Bidirectional 是预言者模式。

还可以利用CNN理解句子！

- **CNN对句子的理解：** 不同长度的局部特征拼凑起来，提供不同的视角，用于构建全面的句子理解。
- **主要在Encoder步骤上使用：** 通过并行的卷积计算，加速对句子的理解过程。


### Transformer注意力模型：信息交汇的奇迹

他是一种自监督学习。

- **Q-K-V（Query-Key-Value）：** Transformer模型采用自注意力机制，通过Q-K-V的方式实现对输入序列的全局理解。

它的语言模型和特点主要有以下几点：

- **因果语言模型和遮盖语言模型：** Transformer通过因果语言模型和遮盖语言模型进行自监督学习，提高模型的表达能力。
- **位置编码：** 为保留输入序列的顺序信息，Transformer引入了位置编码。
- **残差连接和层归一化：** 通过残差连接和层归一化，保证模型的训练效果和深度。

关于编码器-解码器结构：

- **Encoder-Decoder Architecture：** Transformer常用于序列到序列的任务，包含一个编码器和一个解码器，适用于机器翻译等任务。

Transformer的成功与应用主要有：

- **多头注意力和位置感知的前馈网络：** Transformer通过多头注意力和位置感知的前馈网络，提高模型的表达能力和效果。以下是Transformer的一些显著特点：

1. **自注意力机制：** Transformer通过自注意力机制实现对输入序列不同位置之间关系的捕捉。相较于传统的循环神经网络，它可以同时考虑整个输入序列，大大提高了模型的并行性和学习能力。

2. **多头注意力：** 为了增加模型的表达能力，Transformer引入了多头注意力机制。每个头学习不同的关系，通过线性变换后拼接，进一步丰富了模型的特征表示。

3. **位置编码：** 由于自注意力机制无法处理输入序列的顺序，Transformer引入了位置编码，保留了输入序列的位置信息。采用正余弦波的形式的编码，避免了复杂的处理过程。

4. **残差连接和层归一化：** 在每个子层后都包含了残差连接和层归一化，有助于训练深层网络。这使得Transformer在处理大规模数据时更为稳定。

5. **编码器-解码器结构：** Transformer常用于序列到序列的任务，例如机器翻译。其编码器用于处理输入序列，解码器用于生成输出序列，这种结构在NLP领域取得了显著的成功。

### 预训练语言模型算是一种知识的传承

- **信息编码：** 预训练语言模型通过大量数据的信息编码，实现了对丰富知识的储存。
- **知识迁移：** 模型迁移利用前人的知识，大模型比小模型更容易进行迁移。

简单提一下预训练模型中的Encoder和Decoder：BERT与GPT！

- **BERT（Bidirectional Encoder Representations from Transformers）：** 谷歌提出的Encoder模型，专注于单纯的句子理解和文本处理。
- **GPT（Generative Pre-trained Transformer）：** 微软的OpenAI提出的Decoder模型，专注于文本生成任务。通过微调和fine-tuning，可制作个人助手等应用。

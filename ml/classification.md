## 机器学习算法之分类问题：对数变换，逻辑回归

时间宝贵，只总结关键。

### 一个充满分类的世界

从某种程度上来说，回归也是一种分类。这个世界在无限小的量子维度中，也是离散的，分类就是指，对离散值结果的预测。

### 分类问题有哪些类型

- 二元分类（binary）：猫狗，是否得某种疾病。
- 多元分类（multi-class）：多类别分类，为二元分类设计的算法可以适用于多类问题。比如苹果和不是苹果，就可以将苹果从众多水果中分出来。这是一种策略。
- 多标签（multi-label）分类：一个对象可能包含很多属性，这就是多标签，比如一篇新闻报道，可能包含生活，萌宠，百科等很多标签。
- 分类不平衡（inbalanced）：生活中很多问题是分类不平衡的，造物者可以将猫狗类别造的尽量平衡，但是总有很多偏差和小案例，比如欺诈检测，或者恶心肿瘤稀少案例。这种分类需要用到采样策略，让模型看到更多的案例来进行学习。
- 多输出多类（multioutput-multiclass）分类（多任务（multi-task）分类）：这种类型任务较多，比如针对一张图，需要识别出当中的物体，还需要识别出当中的人的情感，还想要识别其暴力级别，那么这就是一个多任务分类问题了。

### 名字迷惑的逻辑回归

逻辑回归是用于分类算法的基本算法，Logit回归，最大熵分类或对数几率回归，线性分类器，它有很多名字，但是回归两个字让很多初学者迷惑。

Logit图像是那条有名的S曲线：Sigmoid function。

它主要用于解决二分类问题，通过估计一个样本属于某个类别的概率来进行分类。毕竟这是一个概率的世界。

**对数几率变换**

首先有一个几率 odds = p / 1 - p

对数几率是指事件发生的概率和事件不发生的概率之比的自然对数。也就是：Logit(p) = log(p / 1 - p)

p 是指事件发生的概率，对数几率可以取任意实数值范围，而不受原始概率值范围的限制。

为什么会有这个公式呢，它的实际意义其实在于将事件发生的概率转换为一个线性形式，从而可以更容易地进行建模和分析。

**sigmoid**：

假设我们要求的实际值为z（W^T*X +b），这个z就是我们拟合了机器学习模型后得到的那个看不太懂的值，对数几率，此处就是z = Logits(p)

由于Logits(p) = log(p / 1 - p) = z，关注后半部分，推导出 p / 1 - p = e^z

p = 1 / (1 + e^(-z))

也就是sigmoid函数。

sigmoid函数的逆函数就是对数几率函数，这意味着，在逻辑回归中，我们通常使用对数几率函数来获得模型的原始输出，然后用sigmoid函数来将输入的线性组合映射到一个0到1之间的概率值。在sigmoid函数的图像中，x轴就是logits，y轴映射的0到1就是logits的逆运算，得到最终我们想要的概率，进而得到分类结果。

使用一对多策略和softmax策略，就可以用逻辑回归解决多分类问题，二元逻辑回归模型还可以扩展到多标签输出。

在scikit learn中使用如下简单代码就可以实现逻辑回归training：

```python
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=0).fit(X_train, y_train)
```

### 一些再次学习的笔记

- 支持向量机仍然是明星算法，核技巧引人注目。
- 决策树算法：最大信息增益，if-then，CART又叫分类和回归树，基尼指数。
- K邻近算法的升级版本：
  - 蛮力搜索算法：是KNN的最简单实现，它涉及计算测试样本与所有训练样本之间的距离，然后选取距离最近的K个样本进行投票。尽管蛮力搜索易于理解和实现，但对于大型数据集来说，计算成本高昂，效率较低。
  - KD树：是一种二叉树数据结构，用于组织数据以支持高效的KNN搜索。它的基本思想是递归地将数据集划分成嵌套的超矩形区域，直到每个区域只包含一个样本。在搜索时，KD树可以通过剪枝和减少搜索空间的方式，显著减少搜索时间。KD树特别适用于低维度数据集。
  - 球树（ball-tree）：是另一种用于加速KNN搜索的数据结构。与KD树不同，球树在构建时不断地将数据集划分成球形区域。在搜索时，球树根据球形区域的相交关系来确定是否需要继续探索某个区域。球树适用于高维度数据集，尤其是当数据集具有不规则分布时。
  - 这些算法都是KNN的变体，旨在提高搜索效率并降低计算成本。在实际应用中，可以根据数据集的特点和算法的性能选择合适的算法来实现KNN分类。
- 集成算法分类算法是分类器的投票表决。

### 巩固贝叶斯定理

贝叶斯定理是概率论中的一项基本定理，它描述了在已知先验信息的情况下，如何通过新的证据来更新我们对事件的信念。该定理以托马斯·贝叶斯（Thomas Bayes）的名字命名，他首次提出了一种通过观察到的数据来更新对事件概率的信念的方法。

贝叶斯定理的表达式如下：

P(A|B) = P(B|A) x P(A) / P(B)

其中：
- P(A|B) 表示在给定观测到事件 B 后，事件 A 发生的条件概率，也称为后验概率。
- P(B|A) 表示在事件 A 发生的条件下，事件 B 发生的概率，也称为似然度。
- P(A) 和 P(B) 分别是事件 A 和事件 B 的先验概率，即在考虑任何新的观测数据之前，我们对事件 A 和事件 B 的概率的初始信念。

贝叶斯定理的核心思想是通过先验概率和似然度来计算后验概率，从而更新我们对事件的信念。这种更新是基于观测到的新证据，而不是依赖于大量的先验信息。

贝叶斯定理在许多领域中都有广泛的应用，特别是在机器学习和统计推断中。例如，在贝叶斯统计中，我们可以使用贝叶斯定理来估计参数的后验分布，进而进行参数推断；在贝叶斯分类中，我们可以使用贝叶斯定理来计算给定类别下观测数据的后验概率，从而进行分类决策。

这个算法在NLP和垃圾邮件检测中非常有用。

### 再谈过拟合和欠拟合

偏差-方差（Bias-Variance）权衡是机器学习中一个重要的概念，用于描述模型的泛化误差（generalization error）与其复杂性之间的关系。

- **偏差（Bias）**指的是模型在训练集上的预测值与实际值之间的差异，即模型的拟合能力。高偏差意味着模型对训练数据的拟合能力较差，很可能出现欠拟合（underfitting）的情况，即模型无法捕捉数据中的真实模式。

- **方差（Variance）**指的是模型在不同训练集上预测值的变化程度，即模型的波动性。高方差意味着模型对训练数据的变化敏感，很可能出现过拟合（overfitting）的情况，即模型过度学习了训练数据中的噪声或特定的样本。

偏差-方差权衡可以用来解释模型的泛化能力。在实际应用中，我们希望模型具有足够的拟合能力（低偏差），同时又能对训练数据的变化具有一定的鲁棒性（低方差）。然而，通常情况下，降低偏差会增加方差，反之亦然，这就构成了偏差-方差权衡。

在实践中，通过调整模型的复杂度（例如调整模型的参数、增加或减少特征、使用正则化等方法），我们可以尝试找到一个合适的平衡点，以最小化模型的总体泛化误差。


### 一些代码

支持向量机的sklearn实现：

```python
from sklearn.svm import SVC
clf = SVC(C = 1, kernel = "linear").fit(X_train, y_train)
```

决策树算法：

```python
from sklearn import tree
clf = tree.DecisionTreeClassifier().fit(X_train, y_train)
```

贝叶斯算法：

```python 
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
clf = gnb.fit(X_train, y_train)
```

k邻近算法：

```python
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3,algorithm="kd_tree")
clf = neigh.fit(X_train, y_train)
```

算法可以使用如下选择：
- ball_tree.：它将使用球树算法。
- kd_tree.：它将使用KD-Tree算法。
- brute：它将使用暴力搜索。
- auto：它将尝试根据传递给 fit 方法的值来决定最合适的算法。

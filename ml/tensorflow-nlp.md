## Tensorflow 自然语言处理流程以及kaggle入门级项目代码实践

---

### 自然语言处理的主要应用领域

机器翻译，文本处理，情感分析，文本分类。

### 在 tf 中的主要处理流程是什么

- 数据准备
- 数据处理，词向量化，词嵌入
- 建模（包括从 hub 提取好用的迁移模型）
- 编译模型
- 拟合模型
- 通过测试集预测和查看结果
- 更多的模型试验和最终模型比较
- 查看错误的预测，考虑如何通过标签修改提升模型性能
- 应用部署

### 词向量化和词嵌入

对于计算机来说，一切都是数字，所以 nlp 也不例外。

词向量是NLP中的基础概念之一，它通过非监督学习方式获取词汇之间的相关性信息。以角度的方式测量词向量之间的距离，能够告诉我们词的频率性特征。

词转数字就是词向量化，很好理解，最简单的向量化，就像是 one-hot 的方法，给每个单词一个数字，然后交给一个向量，给他维度。

词嵌入可以理解为，不只是看一个单词，而是解析单词和单词之间的关系的矩阵，同时还有另一个矩阵在内部作用，context 矩阵，该矩阵装着上下文信息，通过每一个批次的处理，使得两个矩阵的数字不断调整，让关系更近的词汇更近，更远的词汇更远。通过学习，提高模型精度。

### 1 维 CNN 在处理时间序列和序列化数据中表现出色

虽然说 CNN 一说起来就是图像识别领域，但是在序列化数据中，使用 1 维的 CNN 模型表现也非常出色。

### sklearn也有自己的自然语言处理模型

基线模型朴素贝叶斯只是用了 tf-idf 处理单词，表现就非常突出，在基础构架中，打败它的只有迁移学习模型 USE。

**TF-IDF算法**简单来说，就是词频和逆文档频率的乘积。词频主要是指token在单独文档中出现的频率，逆文档频率主要是指在整个文档集中出现的频率。

如果一个单词在一个文档中多次出现，那么它可能比较重要，但是如果在整个文档集中都普遍出现那么可能就没那么重要了。IDF的计算，是总文档数/包含该词的文档数（+1防止除数为0）取对数，如果出现的越多，idf就越小，最后的结果就越小，反之越高，那么这个词在那篇文档中的地位就越重要了。

有时候这个算法不单单用于单独单词，还可能是bigrams两个单词，或者trigrams三个单词一组的。

通过该算法可以模拟建设一个小型的搜索引擎。

用tfidf实现搜索的几个步骤：

- **TF-IDF的cos距离：** 计算文章和问句的TF-IDF值，通过cos距离比较向量的夹角，实现文本搜索。
- **关键词提取：** 通过TF-IDF得分最高的词语，提取关键词。
- **企业级并行搜索：** Elastic Search等工具的应用，利用BM25算法进行集群搜索。

关于sklearn的稀疏矩阵：

- **sklearn的使用：** 使用sklearn库进行TF-IDF算法的实现，稀疏矩阵的存储优化。
- **稀疏矩阵：** 具有更多零元素的矩阵，在存储和处理上具有一些优势，适用于大规模数据集。

#### word2vec：词语特征的奇迹

- **直接应用：** word2vec可以将词向量当成词语特征输入到另一个模型中，实现多种任务，如情感分析、命名实体识别等。
- **加减法运算：** 还能进行加减法运算，通过计算相似性找到相近词语。
- **余弦相似度和负采样：** 利用余弦相似度衡量词向量之间的相似性，同时通过负采样机制提高训练效率。

#### CBOW与Skip-Gram

- **CBOW（Continuous Bag-of-Words）：** 类似于一个连续的词袋，通过前后文窗口移动训练，预测目标是中间的词。但其空间上的向量相加在直观上存在问题，通常被当做预训练模型，用于进一步训练句向量。

这相当于是一种**概率视角**下的语言模型，在t+1个单词和t-1个单词出现的情况下单词t出现的概率是什么。这么想一下其实所有的学习都是在找一个发生的概率。P(wt | wt-1, wt+1) 本质上是一种条件语言模型。在发生前后单词的情况下，中间单词发生的概率。

改变一下视角，将前面的N个单词作为条件，下一个单词发生的概率一样是条件语言模型，而且发现了吗，它是一个马尔可夫模型，或者马尔可夫链。

- **Skip-Gram：** 在理解上更为合理，通过一个词预测其前后的词，适用于一词多义的情况。

#### 两个矩阵：嵌入矩阵与上下文矩阵

词向量的表示通常使用两个矩阵，嵌入矩阵（embedding）和上下文矩阵（context），共同构建出词汇的丰富语义信息。在模型中的目的，就是让这两个矩阵不断进行优化，从而让模型学到丰富的上下文信息的空间表示。

### 句向量：时间序列的循环奇迹

利用循环神经网络（RNN）进行训练！

- **Encoding/Decoding：** RNN以其擅长的时间序列预测能力，通过Encoding和Decoding过程实现对句向量的学习。
- **Seq2Seq：** 在NLP中，Seq2Seq模型利用RNN进行向量化和顺序理解，适用于翻译、情感分析、对话生成等任务。

seq2seq在训练期间，执行两项任务：

输入任务：从输入序列中提取有用信息

输出任务：使用输入序列和输出序列中先前单词的信息计算每个输出时间步的单词概率

- **Beam Search：** 通过关注当前候选词的N个最优策略，提高找到全局最优策略的概率。

另外这三个模型真的挺好玩的 LSTM，GRU，Bidirectional。

LSTM 是回忆过去的记忆，GRU 是控制是否遗忘记忆，Bidirectional 是预言者模式。

还可以利用CNN理解句子！

- **CNN对句子的理解：** 不同长度的局部特征拼凑起来，提供不同的视角，用于构建全面的句子理解。
- **主要在Encoder步骤上使用：** 通过并行的卷积计算，加速对句子的理解过程。


### Transformer注意力模型

他是一种自监督学习。

- **Q-K-V（Query-Key-Value）：** Transformer模型采用自注意力机制，通过Q-K-V的方式实现对输入序列的全局理解。

它的语言模型和特点主要有以下几点：

- **因果语言模型和遮盖语言模型：** Transformer通过因果语言模型和遮盖语言模型进行自监督学习，提高模型的表达能力。
- **位置编码：** 为保留输入序列的顺序信息，Transformer引入了位置编码。
- **残差连接和层归一化：** 通过残差连接和层归一化，保证模型的训练效果和深度。

关于编码器-解码器结构：

- **Encoder-Decoder Architecture：** Transformer常用于序列到序列的任务，包含一个编码器和一个解码器，适用于机器翻译等任务。

Transformer的成功与应用主要有：

- **多头注意力和位置感知的前馈网络：** Transformer通过多头注意力和位置感知的前馈网络，提高模型的表达能力和效果。以下是Transformer的一些显著特点：

1. **自注意力机制：** Transformer通过自注意力机制实现对输入序列不同位置之间关系的捕捉。相较于传统的循环神经网络，它可以同时考虑整个输入序列，大大提高了模型的并行性和学习能力。

2. **多头注意力：** 为了增加模型的表达能力，Transformer引入了多头注意力机制。每个头学习不同的关系，通过线性变换后拼接，进一步丰富了模型的特征表示。

3. **位置编码：** 由于自注意力机制无法处理输入序列的顺序，Transformer引入了位置编码，保留了输入序列的位置信息。采用正余弦波的形式的编码，避免了复杂的处理过程。

4. **残差连接和层归一化：** 在每个子层后都包含了残差连接和层归一化，有助于训练深层网络。这使得Transformer在处理大规模数据时更为稳定。

5. **编码器-解码器结构：** Transformer常用于序列到序列的任务，例如机器翻译。其编码器用于处理输入序列，解码器用于生成输出序列，这种结构在NLP领域取得了显著的成功。

### 预训练语言模型算是一种知识的传承

- **信息编码：** 预训练语言模型通过大量数据的信息编码，实现了对丰富知识的储存。
- **知识迁移：** 模型迁移利用前人的知识，大模型比小模型更容易进行迁移。

简单提一下预训练模型中的Encoder和Decoder：BERT与GPT！

- **BERT（Bidirectional Encoder Representations from Transformers）：** 谷歌提出的Encoder模型，专注于单纯的句子理解和文本处理。
- **GPT（Generative Pre-trained Transformer）：** 微软的OpenAI提出的Decoder模型，专注于文本生成任务。通过微调和fine-tuning，可制作个人助手等应用。

### 文本分类项目

**1. input数据是什么样的**

[batch_size, embedding_size]批次大小和嵌入矩阵的大小。

**2. 各个层的解释**

`input`：字符串输入。

`text_vectorizer`：词的处理，词向量化，或者也叫tokenization。

`embedding`：词嵌入，将离散的符号映射到连续的向量空间的工作。

`LSTM`层：带有tanh激活的层。

`output`：使用sigmoid函数输出结果。

中间还会根据需要嵌入一些隐藏层和全联接层。

**3. Kaggle相关项目链接**

[Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started)是一个使用自然语言处理对推特文本进行分类，是否含有关于灾难的言论。这个项目是一个kaggle的入门级自然语言处理项目。

数据包括以下项目：

- id- 每条推文的唯一标识符
- text - 推文的文本
- location - 推文发送的位置（可能为空）
- keyword - 推文中的特定关键字（可能为空）
- target- 仅在train.csv中，这表示推文是否与真正的灾难有关 (1) 或不是 (0)

除了自己进行分析和学习之外，页面中还有很多别人的代码可以进行学习，kaggle大法好。

**4. 用pandas处理数据**

```python
train_df = pd.read_csv("nlp_data/train.csv")
test_df = pd.read_csv("nlp_data/test.csv")
```

对数据进行打乱抽样：

```python
# shuffled data frac=1，1是百分比表示抽取整个样本
train_df_shuffled = train_df.sample(frac=1, random_state=42)
```

通过数据探索`train_df.target.value_counts()`，查看数据是否是平衡数据。可以得到标签为1的数据有3000多，标签为0的数据有4000多，相对来说是平衡数据。

通过len方法查看训练集和测试集的大小，因为是kaggle的官方数据，所以大体上很合理，训练集7000多，测试集3000多。

**5. 数据集的分割**

一般来说数据集的模型拟合需要进行损失计算，那么就需要训练集和测试集，虽然看到kaggle给的数据集中test.csv文件是叫做，测试集，但是它是没有标签的，所以在模型训练中，可以说是没用的，所以需要我们手动分割数据，将train.csv分割为我们需要的训练和测试集。

使用sklearn进行分割：

```python
# split data by sklearn train_test_split
from sklearn.model_selection import train_test_split
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    train_df_shuffled["text"].to_numpy(),
    train_df_shuffled["target"].to_numpy(),
    test_size=0.1,
    random_state=42
)
```

**6. Tokenization & Embedding**

自然语言处理（NLP）中两个将文本转化为数字表示的主要概念：

一个是**Tokenization（标记化）**包括词级别的标记化，将单词进行映射，还有字符级别的，字符级别的精度更小，还有子词级别的，相对单词会细分一些但是没有到字母级别的精度。将它们表达为数字，就是标记化。

另一个是**Embeddings（嵌入）**，Embedding是自然语言的一种表示，可以通过学习得到。表示采用特征向量的形式。例如，单词"dance"可能被表示为5维向量[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]。需要注意的是，特征向量的大小是可以调整的。一旦文本被转化为数字（嵌入所需的形式），可以通过嵌入层（例如`tf.keras.layers.Embedding`）将它们传递，并且在模型训练期间将学习嵌入表示。另外还可以**重用预先学习的嵌入**许多在线上存在预先训练的嵌入。这些预训练的嵌入通常是在大量文本语料库（如维基百科）上学到的，因此具有对自然语言的良好基础表示。可以使用预先训练的嵌入来初始化模型，并对其进行微调以适应特定任务。
  
很好的一些阅读资源：

https://jalammar.github.io/illustrated-word2vec/

https://nlp.stanford.edu/projects/glove/

总之，嵌入矩阵和上下文矩阵的优化过程，使得模型学到了在嵌入空间中，有意义的向量表示，使得相似的单词具有相似的表示，这些学到的表示可以被用于各种自然语言处理的任务。



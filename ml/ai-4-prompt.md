## AI 时代提问准则

---
### 什么是Prompt Engineering

- LLM是在数字维度的模型，他们对提问的细节非常敏感，一个细微的计算差别就会有不同的结果产生。
- 自然语言就是LLM的编程语言。就像编程语言之于计算机，就像数学之于这个宇宙。
- LLM的涌现能力（Emergent Abilities）的可能性。在一个点突然爆发。
- LLM的不可解释性，这需要人去弄懂。小的prompt的变化可能会给一个模型带来很大的不同。
- 对自然语言这种编程语言的最大化应用。要想要好的答案，就要有好的问题。
- The hottest programming language is English. - Andrej Karpathy
- Prompt Engineering is a skill, more than a job.

### Break down LLM

- 顺序不同会带来结果的不同。他们依照已有的内容输出。It think as it types - 边打边输出。
- 已输出的内容，会和下一次输入一起作为输入。
- 无非是统计结果的选择和输出。温度指标代表了随机性，随机性是这个世界的确定性。如果你让LLM掷骰子，温度设置为0，那么它会给出training数据中，出现概率最大的那个数字。统计和概率论始终支配着这个世界，LLM在这个世界中，它只是给出了这个世界中它训练数据中，分布概率最高的结果罢了。
- 然而神奇的是，只是设置了概率和输出，LLM可以将离散的结果连接成话，那是这个世界的共同意识。
- Pre-trained 得到 Base model，通过微调得到聊天应用。
- The Reversal Curse：同一个问题反过来问，它一个知道一个不知道了。他知道A是B，但是不知道B是A。

### 挑战简单解释LLM

首先解释范畴：机器学习包含深度学习，深度学习包含自然语言处理也就是NLP，NLP算法产出的模型就是LLM，大语言模型。

不管是哪种都是一种，通过算法 + 数据，产出模型的过程。

什么是模型，让我来举出简单有趣的例子。对于使用模型，并且懂得编程语言的人，解释起来模型就是一个**函数**而已。

---
具体打个比方：

模型的执行包括两个config文件：假设一个是parameters（任何可以读取的文件），一个是run.py

（实际上如果你把Meta公司的llama-2-70b模型安装在自己的计算机上，打开设置文件看到的就是两个文件，一个叫parameters，一个叫run.c，只不过我这里想用Python的代码打比方而已。）

好了那么当我们运行大模型的时候我们是在执行这个run.py文件：

```python
from parameters import param
def run(input):
    init = 1
    result = 0
    for x in param:
        init *= x
        result += init
    return result 
```

仅此而已。虽然这处理很愚蠢，但是用一句话说明就是，parameters文件中只是一组又一组的矩阵。这些矩阵作为一种变换的方式，和你的输入进行相乘和相加的计算，最后吐出结果。只不过我省略了复杂的数学，用直观的方式表示，从直观上来说没错。

在模型使用上，机器学习的基于统计的模型，和现在的LLM基于Transformer算法训练出的模型，在使用他们的时候，都是这样一种输入加处理加输出的方式而已。

以上是对模型的讲解，一句话来说，它就是一种使用用户的输入，进行数学计算后，得到结果返回给用户的过程。没有别的。

---
那么LLM模型的特殊之处是什么？没有特殊之处，因为在计算机的世界，他们训练模型的过程，就是把所有的语言进行从单词到token，再到向量的处理过程。结果来说，就是语言在计算机的世界也是数字，那么事情就简单了，LLM和一般的机器模型没什么区别也是数字的计算。

那为什么最热门的Transformer那么复杂和显得高级，单纯是因为它解决了人类语言中很多不确定的语言联系问题。不过恕我直言，这种解决问题的方法，还是一堆矩阵相乘，完全是线性代数。

硬要说和其他模型的不同，真有一个，那就是你上次输入的内容可以作为下次输出的材料，也就是说你输入的内容越详细，模型的输出效果就越好，这让LLM显得很智能，但其实仅仅是因为它进行判断和计算的数字更多了，它可以为你输出更相关的内容，仅此而已。

---
模型版本有好多，他们的区别是什么？

只要理解了参数越多，也就是刚刚说的parameters文件中的参数，越多预示着模型计算量越多，结果越好，这就是GPT3.5和GPT4的区别。3.5训练完后把参数打包给你用了，然后在3.5的基础上继续增加参数继续训练就能得到更好的模型。

能力越大的模型可以处理的工作就越多，比如语言输出图像，图像识别输出文字，文字输出语音，这种不同类型的转换，就是人们说的多模态。但是作为学习者我们一般只是用用文字，或者批量识别图片打标签这种。

多模态多意义就是希望模型得到像人一样的能力，能看会说还会听会画。

---
拆解ChatGPT概念：

GPT模型和ChatGPT不是一个概念。ChatGPT只是在GPT模型基础上host在网页的应用而已。GPT三个字母代表Generative Pre-trained Transformer。

我们拆解一下：

- Transformer就是几乎现在所有LLM的基础算法。谷歌出品。
- Generative 代表了它就是为了输出的。相反的还有一个编码式，编码代表通过模型训练，将巨大的网页世界的文字都转换成parameters（刚刚比喻的文件又登场了）。
- Pre-trained代表预训练，就代表它是一个基础模型：base-model。这个阶段是没法对话的，它就像一个聪明的哑巴，输出的是很奇怪的内容，比如下图：
![basemodel](basemodel.png)
- Chat，意味着它是在基础模型的基础上,针对特定的交互式对话任务进行了进一步训练和优化。这个过程叫做微调，微调的结果是助手模型：Assistant-model。

付费用户可以在OpenAI后台进行自己的微调，创建自己的助手机器人，比如专门用于翻译的，专门用于写代码的。ChatGPT就是专门用于聊天的助手而已。

什么是微调？就是对模型进行指导，和再次对轻微训练，冻结90%的参数，比如只对输出部分进行指导和规范，进行小范围训练，生成新的调整过的模型，这就是微调。

---
现在为止的模型情报有哪些，我该用什么模型作为我的学习伙伴：

**OpenAI**：众所周知，LLM先锋公司。

- GPT3.5：上一代模型。用于网页应用ChatGPT
- GPT4：最新模型。付费后，可以在ChatGPT网页应用中选择该模型，可以使用API，可以在控制台进行模型微调（也就是用你手中自己的数据，让模型变得更懂你的需求）来创建自己的AI助手。如果你的工作是重复性很多的那种，你用这个助手就基本可以通过助手自动化你的工作，然后做一个躺平的上班族了。
- DALLE：付费模型，在控制台可以选择模型，可以给你画图，也就是生成式算法GAN（只要知道这个算法可以教模型画图就可以了）的成果模型。
- Embeddings：将文字转换为数字，针对哪些像自己进行模型微调的人，因为你要喂给模型数据，就要把你的文字转换为数字。（刚刚说了计算机里都是数字，仅此而已），所以这属于一个用于模型准备的工具（简单理解）。


**Anthropic**：这个公司很有趣，它的创始人是从OpenAI跑出来的，他不满OpenAI的做法，觉得他们处理模型太随便了不安全，所以他自立门户了。他们家的模型是现在非常热门的！！他们的原则是安全，强大，又开放。

- Claude3 sonnet：这就是他们家最新的模型。使用这个模型的网页应用就叫Claude。可以进行文字输出，还可以上传图像进行看图说话。但是我只用它的网页应用，它很聪明。目前为止非常满意，我没有购买pro，所以不知道API和后台情况。但是用于学习伙伴，如果不付费GPT4的话，Claude是一个非常好的选择。我在AWS的Bedrock中也见到了它，它是热门的模型。

**Google**：

- Gimini：网页应用非常好用，精度不错，配合Claude足够了。同时它集成了GoogleMap等多个应用，仿佛只是在Google的众多应用中加了一个工具一样自然。API对于免费账户，每天可以尝试50条，我之前学习完了Gimini的API使用的小课程，还没用完当天的限额。

我喜欢Gimini的地方就是它会告诉我内容出处，我可以进一步调查。

说一件非常有趣的事，我有一次问他GCP和AWS中某个服务的区别，它给我付了链接，GCP的链接显示了，但是AWS的链接它显示链接无效不可显示ww。是商业原因吗哈哈。

**Meta**：

- Llama3：开源最新最强的模型，比GPT4还厉害，之前新闻上很爆炸，甚至可以在我自己的Mac上运行。网站在[这里](https://lmstudio.ai/)检查一下你的机器型号，就可以在本地下载模型，也就是那个parameters文件（我的比喻又出现了），但它真的叫做parameters，而且它的大小是140GB。然后就可以在本地call你的模型了，完全离线，很酷。

**Microsoft**：

- Bing也内置GPT4，毕竟OpenAI也是他们的。但我不常用，所以这里不做解释了可以自己尝试，只要知道它内部也是GPT即可。

总结一下，这些模型都是Transformer算法产出的结果，他们性能好坏，只取决于各个公司的算力和数据。

---
该如何看待LLM模型：

- 学习伙伴：查询工具，文章总结工具，快速获得答案的工具。但他最多只是参谋，给你的建议和结果你要审核再用，但是它的建议采用性很高。
- 它脑中的信息是这个世界正态分布的结果：如果你问他随机掷骰子会出几，它会说4（将随机度调节为最低），为什么？因为这是统计的结果。人们倾向于在充满文字的网络世界中说4，也许吧，总之统计的力量在支配。
- 厉害的博学家但是语言表达能力需要你的指引：所以在你提问的时候，越是详细，得到的答案越好，你的思考方式越是有逻辑，提问越是能引导，就越能得到正确的结果。你就像是大学的博士生导师，面对一个高智商的博士，你的任务就是引出它知识库中的宝藏。比如你要写一个游戏代码，相比较直接让他写，如果你先让他输出编码流程，然后再让他按照流程进行编码的效果会好非常多，这代表着你让他把自己的输出当成输入继续进行创作了。
- 工具而已：很多人害怕AI替代了自己，但是能替代自己的只有会用AI工具的人而已。而不是AI。仅仅是工具而已，不需要理解内部原理，仅仅是使用的话，完全没什么难度。重要的是自己的思考的逻辑性，和通过工具提高学习效率的工具利用而已。

---
虽然理解Transformer让我很快乐，但是没有计算资源的我们也就是只能理解而已。但是有一些基础的知识后，再去学习一些稍稍高级的概念就会更加简单，比如一般人可以处理的微调，Rag，LangChain实装，API使用，都是使用模型的方法。这些是一个工程师应该知道的。

如果手动实装训练模型的过程，当然可以更好的体会，但是对于非AI领域的工程师，知道模型的实质就足够了。

我要一句话总结模型：使用算力和数据，将一组随机的parameter通过不断的loop训练，得到看似可以预测的数据的一组parameters，使用这些parameters，将你的输入进行处理，得到输出。工程师，之需要关注训练后的部分即可了。

这是一个巨大的领域，如此的说明非常粗糙，但是对于一个日常目标是应用研究成果，到实际中去的，工程师来说，首先理解了它的本质，就会让很多问题变得简单了。

---
一些其他的Topic：

- 模型训练被很多人称为炼金术。我觉得确实如此，因为没有任何两个人训练的模型是一样的，显示了这个世界的随机性。
- 深度学习和大模型中完全只是数字计算，线性代数，但是却能得到逻辑合理的结果，没人可以解释，这被叫做涌现。已经超出了科技范畴了。
- 除非你真的感兴趣，我不建议去尝试编码和学习机器学习和深度学习算法，因为很难出成果。如果是小的数据集预测，还是很值得学习机器学习的。深度学习算法虽然厉害，但是也要根据问题选择，有时候传统的机器学习算法更合适，这个世界还没有发明出泛用算法和泛用AI。
- AI的边界，其实就是数学的边界，数学是这个宇宙的编程语言，然而数学都不能解决的问题还非常多，更别说AI了，所以他一点都不可怕，只是一种算法而已。但我觉得最棒的算法永远是这三个：爱因斯坦质能方程，熵增定律，和1+1=2。AI还是弱的。最厉害的还是人脑。



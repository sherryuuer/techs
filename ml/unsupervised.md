## 无监督学习

指从没有标签的数据中进行推断的学习。

无监督学习帮助我们识别数据集中的模式。

数据聚类是无监督学习技术之一，其中单个聚类中的元素具有相同的属性并被分配相同的标签。一些最著名的聚类算法包括：

- K-means算法
- 层次聚类
- DBSCAN聚类

除此之外还有关联规则挖掘，异常检测和降维。

最常听的主要是Kmeans，降维PCA。异常检测也是我经常听到的主题。

## K-Means算法

它是一种迭代算法，如何工作：

- 随机挑选K个质心。
- 通过计算相似度或者距离将每个实例添加到最近的质心范畴内。
- 重新计算每个类别的（叫做簇）质心，继续进行所有实例的归类。
- 重复步骤，直到各个实例的分类不再变化。

它是一种简单可以实现的算法，当然它也有很多缺点，就从第一步初始化开始就有很多漏洞，如何选到最好的质心关系到最后的结果。所有进化出一种K-Means++算法涉及初始化质心，使它们彼此远离。它比随机选择质心要好得多。

通常使用欧氏距离（Euclidean distance）作为数据点之间的距离度量。欧氏距离是指在几何空间中，两个点之间的直线距离。理论上可以使用余弦相似度来替代欧氏距离，但在实际应用中需要注意，余弦相似度适用于表示为向量的数据，因此在使用余弦相似度时，需要将数据点表示为向量形式。此外，余弦相似度不考虑向量的绝对大小，而是关注向量之间的方向，因此在某些情况下，余弦相似度可能更适合于描述数据的相似性。

欧氏距离是计算点之间的距离。余弦相似度关注向量的方向，它是（向量之间的dot运算/各个向量的欧几里得范数相乘）得到的。

### 余弦相似度

**余弦相似度**的推导首先是从空间向量的点乘公式得到的，这就是点乘公式：A · B = ||A|| ||B|| cos(θ)

**点乘公式**描述了两个向量有多大的投影在对方的方向上,如果两向量完全重合,则它们的点乘结果就等于各自模长的乘积,即A·B = ||A|| ||B||。

设有两个n维空间向量A和B: A = (a1, a2, ..., an)，B = (b1, b2, ..., bn)

我们先将它们分别展开到n个基向量的线性组合: 
```txt
A = a1e1 + a2e2 + ... + anen
B = b1e1 + b2e2 + ... + bnen
```
其中e1, e2, ..., en是n维空间的基向量。

根据向量的线性运算法则,我们有:
```txt
A·B = (a1e1+a2e2+...+anen)·(b1e1+b2e2+...+bnen) 
= a1b1(e1·e1) + a1b2(e1·e2) + ... + a1bn(e1·en)
+ a2b1(e2·e1) + a2b2(e2·e2) + ... + a2bn(e2·en)
...
+ anb1(en·e1) + anb2(en·e2) + ... + anbn(en·en)
```

根据基向量的正交性质: ei·ej = 0, (i≠j), ei·ei = 1

上式可以化简为: A·B = a1b1 + a2b2 + ... + anbn = Σ(aibi)  (i=1,2,...,n) 这就是空间向量的点乘公式。

**基向量(Basis Vector)**是构成某一向量空间的一组线性无关的向量。在n维空间中,有n个单位基向量e1,e2,...,en,可以构成该空间的一个基底。
对于2维空间,基向量通常设为: e1 = (1,0), e2 = (0,1)，对于3维空间,基向量通常设为: e1 = (1,0,0), e2 = (0,1,0), e3 = (0,0,1)。可以看出它就是一个矩阵中，主对角线都为1。

基向量有以下几个基本性质:

- 模长为1：例如在3维空间,||e1|| = ||e2|| = ||e3|| = 1
- 两两正交（也就是90度）：任意两个不同的基向量ei和ej的点乘为0,即ei·ej = 0 (i≠j) 这就是所谓的正交性质。
- 基向量可用于线性表示任意向量：任意n维向量A可表示为n个基向量的线性组合: A = a1e1 + a2e2 + ... + anen
- 正是由于基向量的这些特性, 我们可以利用它们构造向量空间, 并在向量空间中研究向量的性质和运算。比如点乘公式的推导,就需要利用基向量的正交性质。

所以基向量是构造和分析向量空间的基础,掌握它们的性质对于线性代数等数学理论至关重要！

搞明白了余弦相似度，真舒服！

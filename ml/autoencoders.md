## 生成式AI：模型原理和自动编码器

---
### 理解判别式模型和生成式模型（Discriminative vs generative models）

判别式（Discriminative）模型和生成式（generative）模型。

Discriminative 模型和 Generative 模型是机器学习中两种不同的建模方法，它们的核心区别在于所关注的任务和所建模的概率分布。

1 - **Discriminative 模型**：

- 基本原理：Discriminative 模型关注的是对观察数据和标签之间的条件分布进行建模，即给定输入数据，预测其对应的标签。因此，它们直接学习并建模了类别之间的边界或决策边界。
- 工作原理：Discriminative 模型通常通过学习条件概率分布 P(Y|X) 来实现，给定输入 X 来预测输出 Y，其中 X 表示输入特征，Y 表示标签或输出变量。这些模型主要关注于给定输入 X 来预测输出 Y，而对输入数据的分布并不关心。常见的 Discriminative 模型包括逻辑回归、支持向量机、决策树、神经网络等。
- 它更适合用于分类，标注等任务。

2 - **Generative 模型**：

- 基本原理：Generative 模型关注的是对观察数据的联合概率分布进行建模，即对观察数据的生成过程进行建模。因此，它们试图理解数据背后的潜在分布和生成机制。
- 工作原理：Generative 模型通常通过学习联合概率分布 P(Xn) 来实现，有些情况中，联合概率分布是X和Y的分布，X和Y分别是数据和标签，但是这里要说的生成式模型是不包括标签的。在生成式模型中，学习的确实是数据的联合概率分布，但这并不意味着这个联合概率分布一定与标签有关。
- 生成式模型学习的是观察到的数据点 x 的联合概率分布 p(x)，其中 x 是输入的数据点。这个概率分布描述了数据点在观察空间中的分布情况，而不包含标签信息。通过学习这个概率分布，生成式模型能够生成新的数据样本，填充缺失值，进行推理和预测等任务，而无需外部标签的指导。在生成式模型中，数据本身“竞争”概率密度，即模型学习如何平衡数据点之间的概率分布，使得观察到的数据点具有较高的概率密度。这意味着模型需要尽可能准确地学习数据的分布，以便能够有效地生成新的数据样本，并与观察到的数据点相符合。
- 常见的 Generative 模型包括朴素贝叶斯、高斯混合模型、潜在语义分析、生成对抗网络等。
- 更适合用于生成新的数据样本，填充缺失值，特征提取等任务。
- PS：联合概率分布是什么：它是指同时描述多个随机变量的概率分布。在一个联合概率分布中，每个可能的事件都与每个变量的值的组合相关联，并且分配了一个概率值。这种概率分布描述了多个变量之间的关系，以及它们如何共同决定观察到的数据。假设有两个随机变量 X 和 Y，它们的联合概率分布可以表示为 P(X, Y)。这个分布可以描述在给定 X 和 Y 的情况下，观察到某个事件的概率。例如，如果 X 表示一个骰子的投掷结果，Y 表示另一个骰子的投掷结果，那么联合概率分布可以告诉我们同时观察到两个骰子的特定组合的概率，如 (X=1, Y=3)、(X=2, Y=5) 等。联合概率分布包含了每个变量的边际概率分布以及它们之间的相互作用。通过联合概率分布，我们可以进行诸如边际化、条件化等操作，来计算关于单个变量或者一组变量的各种概率。

3 - **Conditional Generative 模型**：

- 条件生成模型是另一类模型，试图学习数据的概率分布 p(x) 在标签 y 的条件下的分布。这通常表示为 p(x|y)。在这种情况下，我们再次让数据“竞争”概率密度，但这次是针对每个可能的标签。
- “竞争”的概念。概率密度函数 p 是一个归一化函数，其在整个值域上的积分值等于 1。这意味着在给定一组数据点的情况下，它们的概率密度函数应该总和为 1。在条件生成模型中，我们学习的是数据点在给定标签的条件下的概率密度函数 p(x|y)。因此，对于每个可能的标签，我们都有一个对应的概率密度函数。
- 条件生成模型的目标是通过学习数据在给定标签的条件下的分布来实现各种任务。例如，给定一张图片和一个标签，我们可以使用条件生成模型来生成与该标签相符的新图片；或者在给定一些文本描述和一个标签时，生成与描述和标签相符的新文本。这种模型在图像生成、文本生成等任务中具有广泛的应用。

4 - **p(x|y) = p(y|x) * p(x) / p(y)**：

前面提到的模型们，在一定程度上是相互关联的，考虑贝叶斯定理：p(x|y) = p(y|x) * p(x) / p(y)

这个公式告诉我们，我们可以将每种类型的模型建立为其他类型的模型的组合。

这个公式所表示的贝叶斯定理是基于条件概率和边缘概率之间的关系。在这个公式中，p(x|y) 是在给定标签 y 的情况下观察到数据 x 的条件概率。而 p(y|x) 是在给定数据 x 的情况下观察到标签 y 的条件概率。p(x) 和 p(y) 分别是数据 x 和标签 y 的边缘概率。

这个公式告诉我们，我们可以通过对条件概率和边缘概率进行组合，从而构建出各种类型的模型。例如，我们可以从判别式模型（p(y|x)）出发，结合数据的边缘分布（p(x)）来构建生成式模型（p(x|y)）。或者从生成式模型（p(x|y)）出发，结合标签的边缘分布（p(y)）来构建判别式模型（p(y|x)）。

贝叶斯定理为我们提供了一个框架，说明了各种模型之间的联系和互相转换的可能性。这种联系使得我们能够更加灵活地建立和理解各种类型的模型。真是强大又神奇。

### 理解潜变量模型（Latent variable models）

潜变量模型（Latent Variable Models）是一类统计模型，用于描述观察到的数据与未观察到的潜在变量之间的关系。在潜变量模型中，假设观察到的数据是由潜在变量和随机噪声共同决定的，而潜在变量通常不能直接观察到，需要通过观察到的数据进行推断和估计。

潜变量模型通常用于以下几个方面：数据降维和特征提取，模式识别和分类，填充缺失值，探索数据生成过程。

常见的潜变量模型包括潜在类别模型、潜在因子模型、混合模型等。这些模型在各种领域都有广泛的应用，例如在机器学习、统计学、社会科学、生物医学等领域。潜变量模型为我们提供了一种强大的工具，可以帮助我们理解和利用数据背后的潜在结构和规律。

这里为什么要提起这个概念，因为在机器学习领域，这个模型描述的是**潜在变量的概率分布**，存在于数据点集合的**连续的低维空间**。

有的时候我感觉数学公式让直觉理解变的困难，但是在这个情形下，数学公式贝叶斯，让这里变的清晰明了。

这里的潜变量分布，其实是一种先验分布。整理一下看看：

- 数据集X遵循一种分布p(x)，它是到潜变量分布p(z)的map对应。
- 先验分布 piror distribution p(z) 是对潜变量分布进行建模。
- 似然估计（可能性） likehood p(x|z) 定义了潜变量如何映射到数据点。
- 联合分布 joint distribution p(x,z) = p(x|z)p(z) 是似然估计和先验分布的乘积，本质上它描述了模型。
- 边际分布 marginal distribution p(x)是原属数据分布，它描述了生成数据点的可能性，是模型的最终目标。
- 后验分布 posterior distribution p(z|x)描述了特定数据可以产生的潜在变量。

关注上面步骤中两个步骤其实就是生成和推理的过程。

- 生成 Generation：就是由潜在数据点，计算实际数据点的过程就是p(x|z)似然估计的计算。
- 推理 Inference：就是寻找潜在变量的过程，就是p(z|x)后验估计的计算。

```
      Generation p(x|z)
 -----------------------------
↑                                     ↓
p(z)                            p(x)
↑                                     ↓
 -----------------------------
      Inference p(z|x)
```


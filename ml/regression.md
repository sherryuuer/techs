## 机器学习算法之线性回归

---
### 线性回归重要性

现在还有人学习线性回归？我为什么还在线性回归我都Transformer都学过了。高级算法学完了就不需要学机器学习的入门算法了吗，什么是入门算法。这个世界的法则是什么，回归是否支配了我们的世界。

一天我在推特看到一张图，说在transformer的支配下，传统的机器学习算法没落了（权当胡说八道），然后在那张可视化图中，线性回归模型岿然不动，看来什么都逃不过这个世界回归的法则。

### 一些心得和思考

不再从基础概念开始而是直接写下我总结和补充的一些关于回归的心得：

- 多元线性回归中使用可视化的图标好处多多，比如箱线图帮我看到整体的分布和离异值。

- 特征缩放的好处：
  - 它有助于基于梯度下降的算法更快地收敛。
  - 它有助于基于距离的算法在计算相似度时为每个特征赋予相同的权重。（基于距离的算法考虑数据集实例之间的距离或相似性来进行计算。）
  - 它有助于比较特征重要性。

- 正则化Regularization防过拟合。
  - 岭回归Ridge是一种L2正则化，它的拉姆达参数是可以学习的。（过高会导致欠拟合，过低会导致对参数影响忽略不计。）
  - Lasso回归是一种L1正则化，所以它也可以用来进行特征选择，它会给我们一个稀疏解。
  - Elastic-Net回归则结合了上述两种回归模型。
    - l1_ratio是 ElasticNet 混合参数，其值介于 0 和 1 之间。
    - 当l1_ratio= 0 时，使用 L2 正则化。当 l1_ratio= 1，使用 L1 正则化。当 0 < l1_ratio< 1，使用 L1 和 L2 的组合。


## Amazon Bedrock 项目实践1

---
### Bedrock介绍

Amazon Bedrock 是一项完全托管的服务，可简化生成式人工智能 (Generative AI) 应用程序的开发。它提供以下功能：

- 来自领先 AI 公司的各种高性能基础模型 (FM)，包括 AI21 Labs、Anthropic、Cohere、Meta、Stability AI 和 Amazon 自己的模型等。
- 构建生成式 AI 应用程序所需的一系列广泛功能，包括以下内容。
- 微调：使用自己的数据自定义模型输出。
- 检索增强生成 (RAG)：将知识和数据整合到模型中。
- 托管代理：将模型部署到生产环境。
- 安全性、隐私性。输入的数据不会离开AWS的加密系统，所以私人数据是安全的。

Bedrock 可用于构建各种生成式 AI 应用程序，包括：聊天机器人，内容生成，代码生成，翻译，问答，创意写作，数据分析，等等。

目前（2024年4月）进入服务页面，会有部分区域可用部分区域不可用，可用区域的模型也不一样。所以可以多探索一下。一开始的模型也不是立刻可用状态，需要在左侧找到model列表，然后点击编辑，全选模型，更新请求使用的按钮后方可使用。

在他的游乐场中有很多可以玩的模型，并且给了详细的示例，可以随便逛逛看看什么样的prompt会生成什么样的数据。

**关于参数调节：**

在游玩游乐场中的模型的时候有很多参数可以调节，比如温度，topK采样，topP采样等。他们在内部是如何实现的，我突然很感兴趣所以查了一下。他们都是通过调节模型输出的 *概率分布* 来控制生成结果的多样性和质量。这些技术通常应用于基于概率的生成模型，比如语言模型，其中模型会为下一个词的生成提供一个概率分布。

温度调节是指通过调节模型输出的概率分布的熵来控制生成结果的多样性。具体来说，温度参数 τ 越大，模型输出的概率分布的熵越大，生成的结果越多样化；反之，温度参数越小，生成的结果越趋于确定性。温度调节的内部原理是在模型输出的概率分布上应用一个 softmax 操作，并且通过除以温度参数 τ 来缩放分布的值，然后再进行 softmax 操作，得到新的概率分布。

Top-k 采样是一种用于调节生成结果质量的方法，它通过保留概率最高的前 k 个词来限制生成结果的数量，以及在这 k 个词上重新归一化概率分布，从而提高生成结果的质量。Top-k 采样的内部原理是在模型输出的概率分布上应用一个截断操作，只保留概率最高的前 k 个词，并在这些词上 *重新归一化* 概率分布。这样可以避免生成一些低概率的、不合理的词语。具体来说，对于每个词的概率分布，只保留概率最高的前 k 个词，其他的概率设为0，然后在这 k 个词上重新归一化概率分布。

Top-P 采样（也称为Nucleus采样）是一种与Top-k 采样类似的方法，它也用于控制生成结果的多样性和质量。在Top-P 采样中，不是保留概率最高的前k个词，而是保留累积概率大于 *阈值P* 的词。

具体来说，给定一个阈值P（通常在0和1之间），Top-P 采样会保留累积概率大于P的词，然后在这些词上重新归一化概率分布。这样可以根据生成任务的需要动态地调整词汇量，以保持一定的多样性。

Top-P 采样相比于固定的Top-k值，更加灵活，能够根据上下文动态地选择词汇量，因此在一些生成任务中被广泛使用。
